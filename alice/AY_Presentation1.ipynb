{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "# conda install textblob -c conda-forge\n",
    "from textblob import TextBlob\n",
    "import contractions \n",
    "# import sys  \n",
    "# !{sys.executable} -m pip install contractions\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aliceye/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aliceye/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/aliceye/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and creating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_data = json.load(open('train.json'))\n",
    "train_df = pd.json_normalize(train_json_data)\n",
    "\n",
    "test_json_data = json.load(open('test.json'))\n",
    "test_df = pd.json_normalize(test_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
       "       'request_title', 'requester_account_age_in_days_at_request',\n",
       "       'requester_days_since_first_post_on_raop_at_request',\n",
       "       'requester_number_of_comments_at_request',\n",
       "       'requester_number_of_comments_in_raop_at_request',\n",
       "       'requester_number_of_posts_at_request',\n",
       "       'requester_number_of_posts_on_raop_at_request',\n",
       "       'requester_number_of_subreddits_at_request',\n",
       "       'requester_subreddits_at_request',\n",
       "       'requester_upvotes_minus_downvotes_at_request',\n",
       "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
       "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_cols = ['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
    "       'request_title', 'requester_account_age_in_days_at_request',\n",
    "       'requester_days_since_first_post_on_raop_at_request',\n",
    "       'requester_number_of_comments_at_request',\n",
    "       'requester_number_of_comments_in_raop_at_request',\n",
    "       'requester_number_of_posts_at_request',\n",
    "       'requester_number_of_posts_on_raop_at_request',\n",
    "       'requester_number_of_subreddits_at_request',\n",
    "       'requester_subreddits_at_request',\n",
    "       'requester_upvotes_minus_downvotes_at_request',\n",
    "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
    "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc', 'requester_received_pizza']\n",
    "short_train_df = train_df[available_cols]\n",
    "short_train_df.columns\n",
    "\n",
    "# create new df for if someone did not get a pizza\n",
    "no_pizza_df = short_train_df[short_train_df.requester_received_pizza == False]\n",
    "# create new df for if someone did get a pizza\n",
    "yes_pizza_df = short_train_df[short_train_df.requester_received_pizza == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating stopword list\n",
    "\n",
    "Added a couple of custom stopwords that are frequently used in ROAP requests (e.g. piza, request, hey, hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "# adding some of our own stop words to NLTK's stopword list\n",
    "stopwords_nltk_en = set(stopwords.words('english') + ['though','pizza', 'request', 'hey', 'hi'])\n",
    "# create stop word list for punctuation\n",
    "stopwords_punct = set(punctuation)\n",
    "# combine the three stopwords lists\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create text preprocessor\n",
    "\n",
    "Added the ability to replace contractions with their proper form and remove URLS to the text preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    # added functions to replace contractions with their proper form\n",
    "    # and remove any URLs\n",
    "    processed_list = [word for word in lemmatize_sent(contractions.fix(re.sub(r'http\\S+', '', text)))\n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]\n",
    "    processed_text = ' '.join(processed_list) \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TFIDFVectorizer for Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bigram TDIFD vectorizer \n",
    "tf_bigram = TfidfVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (2, 2))\n",
    "short_train_df_tf_bigram = tf_bigram.fit_transform(short_train_df['request_text_edit_aware'])\n",
    "\n",
    "# average frequency of ngrams\n",
    "avg_values = short_train_df_tf_bigram.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = tf_bigram.vocabulary_\n",
    "df_bigram = pd.DataFrame(sorted([(avg_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'sum frequency', 1:'bigram'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum frequency</th>\n",
       "      <th>bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.409068</td>\n",
       "      <td>pay forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.213445</td>\n",
       "      <td>college student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.103988</td>\n",
       "      <td>return favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.499367</td>\n",
       "      <td>sob story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.310112</td>\n",
       "      <td>bank account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.238537</td>\n",
       "      <td>lose job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.831305</td>\n",
       "      <td>papa john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.485988</td>\n",
       "      <td>pay back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.047579</td>\n",
       "      <td>couple day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.012277</td>\n",
       "      <td>pay week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.990419</td>\n",
       "      <td>pay friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.852470</td>\n",
       "      <td>pay rent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.710155</td>\n",
       "      <td>random act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9.017266</td>\n",
       "      <td>money food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.903217</td>\n",
       "      <td>make day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.728274</td>\n",
       "      <td>find job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.478186</td>\n",
       "      <td>week pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.284956</td>\n",
       "      <td>spend money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.246596</td>\n",
       "      <td>week ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.242526</td>\n",
       "      <td>food house</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sum frequency           bigram\n",
       "0       43.409068      pay forward\n",
       "1       20.213445  college student\n",
       "2       17.103988     return favor\n",
       "3       16.499367        sob story\n",
       "4       15.310112     bank account\n",
       "5       14.238537         lose job\n",
       "6       11.831305        papa john\n",
       "7       11.485988         pay back\n",
       "8       10.047579       couple day\n",
       "9       10.012277         pay week\n",
       "10       9.990419       pay friday\n",
       "11       9.852470         pay rent\n",
       "12       9.710155       random act\n",
       "13       9.017266       money food\n",
       "14       8.903217         make day\n",
       "15       8.728274         find job\n",
       "16       8.478186         week pay\n",
       "17       8.284956      spend money\n",
       "18       8.246596         week ago\n",
       "19       8.242526       food house"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bigram[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling using LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "After using the TDIDF vectorizer to create bigrams, we looked into whether similar bigrams can be grouped into topics. One method we used for topic modeling is LDA.\n",
    "\n",
    "LDA is an iterative probabilistic model that creates topics using two probability values: P(Word|Topic) and P(Topic|Document). We set the LDA model tocreate 10 topics. First, LDA will randomly assign bigrams to 10 topics. Then it'll use the two probability values to reassign bigrams to topics. The reassignment step will happen over and over again until the model converges. This results is the final 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: pay forward, lose job, return favor, college student, sob story\n",
      "Topic #1: pay forward, money friday, lose job, papa john, sob story\n",
      "Topic #2: pay forward, college student, sob story, hungry college, ramen noodle\n",
      "Topic #3: pay forward, college student, find job, job pay, return favor\n",
      "Topic #4: pay forward, college student, return favor, sob story, bank account\n",
      "Topic #5: pay forward, college student, bank account, love forever, return favor\n",
      "Topic #6: pay forward, papa john, sob story, pay rent, story random\n",
      "Topic #7: pay forward, pay week, college student, sob story, bank account\n",
      "Topic #8: pay forward, bank account, sob story, return favor, money food\n",
      "Topic #9: pay forward, return favor, college student, lose job, bank account\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topics for TFIDF trigrams using LDA\n",
    "lda = LatentDirichletAllocation(n_components=10)\n",
    "pipe_lda = make_pipeline(tf_bigram, lda)\n",
    "pipe_lda.fit(short_train_df['request_text_edit_aware'])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(lda, tf_bigram.get_feature_names(), n_top_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling using NMF (Non-negative Matrix Factorization)\n",
    "\n",
    "Another topic modeling method we used was NMF. NMF is a linear algebreic model that moves from higher dimensionality to lower dimensionality. It makes a given document-word matrix (our TDIDF matrix) and factors it into two lower dimensional forms: one is a vector of topics and another is a matrix of topic weights for each document. The two lower dimensionality forms are calculated by iterating over them until them optimize and coverge.\n",
    "\n",
    "Below we used NMF to create 10 topics like we did above with LDA modeling. From NMF we saw topics that seemed to make more sense and have less repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: pay forward, promise pay, forward money, forward pay, forward feel\n",
      "Topic #1: college student, broke college, hungry college, poor college, student pay\n",
      "Topic #2: return favor, gladly return, favor paycheck, love return, title return\n",
      "Topic #3: sob story, story hungry, title pretty, pretty sob, hungry bore\n",
      "Topic #4: lose job, job week, week ago, job month, job couple\n",
      "Topic #5: pay week, forward pay, week pay, week buy, reciprocate pay\n",
      "Topic #6: bank account, account information, cent bank, past week, dollar bank\n",
      "Topic #7: pay back, pay friday, money food, eat today, day pay\n",
      "Topic #8: papa john, gift card, domino hut, full time, domino papa\n",
      "Topic #9: couple day, make day, past couple, live ramen, ramen past\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## topics for TFIDF trigrams using NMF\n",
    "nmf = NMF(n_components=10)\n",
    "pipe = make_pipeline(tf_bigram, nmf)\n",
    "pipe.fit(short_train_df['request_text_edit_aware'])\n",
    "print_top_words(nmf, tf_bigram.get_feature_names(), n_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: pay forward, forward money, forward feel, promise pay, grateful pay\n",
      "Topic #1: sob story, money food, title pretty, find heart, hungry nice\n",
      "Topic #2: college student, couple day, poor college, live ramen, past couple\n",
      "Topic #3: pay week, reciprocate pay, leave house, cereal egg, house mood\n",
      "Topic #4: return favor, pay friday, love return, day pay, fee day\n",
      "Topic #5: lose job, make day, job week, week ago, food low\n",
      "Topic #6: bank account, empty bank, pay bank, dollar bank, account pay\n",
      "Topic #7: day love, eat day, meal day, love nice, love eat\n",
      "Topic #8: papa john, john town, code papa, gift card, hut papa\n",
      "Topic #9: food fridge, news bad, landlord power, complex careful, fridge back\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## topics for TFIDF trigrams using NMF for those who did get pizza\n",
    "nmf = NMF(n_components=10)\n",
    "pipe = make_pipeline(tf_bigram, nmf)\n",
    "pipe.fit(short_train_df.request_text_edit_aware[short_train_df.requester_received_pizza == True])\n",
    "print_top_words(nmf, tf_bigram.get_feature_names(), n_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: pay forward, promise pay, money pay, forward pay, hungry money\n",
      "Topic #1: college student, broke college, hungry college, student pay, break college\n",
      "Topic #2: return favor, gladly return, favor paycheck, title return, favor pas\n",
      "Topic #3: sob story, story hungry, hungry bore, random act, give sob\n",
      "Topic #4: lose job, job month, job couple, job week, month ago\n",
      "Topic #5: pay back, pay friday, love pay, pay day, hungry pay\n",
      "Topic #6: bank account, account information, cent bank, past week, make day\n",
      "Topic #7: money food, eat today, food eat, couple day, day ago\n",
      "Topic #8: pay week, forward pay, week buy, week pay, buy pay\n",
      "Topic #9: papa john, gift card, domino hut, full time, alfys papa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## topics for TFIDF trigrams using NMF for those who did not get piza\n",
    "nmf = NMF(n_components=10)\n",
    "pipe = make_pipeline(tf_bigram, nmf)\n",
    "pipe.fit(short_train_df.request_text_edit_aware[short_train_df.requester_received_pizza == False])\n",
    "print_top_words(nmf, tf_bigram.get_feature_names(), n_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
