{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simranbhatia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simranbhatia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/simranbhatia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/simranbhatia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/simranbhatia/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "#import contractions \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.tree\n",
    "import re\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases, KeyedVectors, Word2Vec\n",
    "import gensim.downloader\n",
    "\n",
    "from scipy import stats\n",
    "from statistics import mean \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import xgboost as xgb\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Introduction\n",
    "\n",
    "We have a dataset in JSON format containing information about the user activity in a reddit forum **ROAP**. Our feature set comes from a bunch of user features noted at the time of posting user comments in the forun alogside the titel and text they chose. The response variable is whether the requester received the pizza or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/random-acts-of-pizza/train.json') as f:\n",
    "    train_json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d0ca947ab42d>:1: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  df = pd.io.json.json_normalize(train_json_data)\n"
     ]
    }
   ],
   "source": [
    "df = pd.io.json.json_normalize(train_json_data) \n",
    "df.head()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data rows in full training set: 4040\n",
      "Total data columns in full training set: 32\n"
     ]
    }
   ],
   "source": [
    "print('Total data rows in full training set: {}'.format(df.shape[0]))\n",
    "print('Total data columns in full training set: {}'.format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Splitting training set into training and validation\n",
    "\n",
    "As we don't have labeled test set, we have to split existing training data into training and validation set in order to evaluate our model performance. We have already split the train and validation in 90/10 ratio and saved as csv files. We read them directly in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in training and test data\n",
    "train, val = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    2746\n",
      "True      890\n",
      "Name: requester_received_pizza, dtype: int64\n",
      "False    300\n",
      "True     104\n",
      "Name: requester_received_pizza, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.requester_received_pizza.value_counts())\n",
    "\n",
    "print(val.requester_received_pizza.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting relevant columns (present in test data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = ['request_title', 'request_text_edit_aware' ,\n",
    "                 'requester_number_of_posts_on_raop_at_request', \n",
    "                'requester_number_of_subreddits_at_request', \n",
    "                 'unix_timestamp_of_request', \n",
    "                'requester_account_age_in_days_at_request',\n",
    "                'requester_subreddits_at_request',\n",
    "                'requester_upvotes_minus_downvotes_at_request', \n",
    "                'requester_upvotes_plus_downvotes_at_request']\n",
    "\n",
    "y = 'requester_received_pizza'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df(data, y_cols, x_cols):\n",
    "    return data[[y_cols] + x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = subset_df(data=train, y_cols=y, x_cols=relevant_cols)\n",
    "val = subset_df(data=val, y_cols=y, x_cols=relevant_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simple Text and Non-text features (function repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subreddits at request \n",
    "def num_subreddit_cuts(data, col, grid, lab):\n",
    "    return pd.cut(data[col], grid, labels=lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_subreddit_dummy(data, colname):\n",
    "    \"\"\"\n",
    "    data (pandas dataframe): input data frame\n",
    "    colname (str): original column name\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(data=data, columns=[colname], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_posts_roap(data, colname, id_colname,th=0):\n",
    "    \"\"\"\n",
    "    data (pandas dataframe): input data frame\n",
    "    colname (str): original column name\n",
    "    id_colname (str): identifier for the created feature\n",
    "    th (int): threshold for creating binary feature\n",
    "    \"\"\"\n",
    "    new_colname = id_colname + '_' +colname\n",
    "    feat = np.where(data[colname] == th, 0, 1)\n",
    "    data[new_colname] = feat\n",
    "    del data[colname]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_request_hour_feature(data, time_col, interval):\n",
    "    \"\"\"\n",
    "    data (pandas dataframe): input data frame\n",
    "    time_col (str): time indicator column\n",
    "    interval (int): number of intervals \n",
    "    \"\"\"\n",
    "    data[time_col] =  pd.to_datetime(data[time_col],unit='s')\n",
    "    hour_bin_int = pd.cut(data[time_col].dt.hour, interval)\n",
    "    hour_col_name = time_col + '_' + str(interval)\n",
    "    data[hour_col_name] = str(hour_bin_int).replace(\",\",\"-\")\n",
    "    data = data.drop(time_col, axis=1)\n",
    "    return pd.get_dummies(data=data, columns=[hour_col_name], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_token_count(v):\n",
    "    result = []\n",
    "    for line in v:\n",
    "        if line != line:\n",
    "            result.append(1)\n",
    "            continue\n",
    "        result.append(len(word_tokenize(line)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengths_feature(data):\n",
    "    return [len(t) for t in data['request_text_edit_aware']]\n",
    "\n",
    "def num_caps_feature(data):\n",
    "    return [sum(1 for word in x.split() for c in word if c.isupper()) for x in data['request_text_edit_aware']]\n",
    "\n",
    "def caps_vs_lengths_feature(data):\n",
    "    return [sum(1 for word in x.split() for c in word if c.isupper())/\n",
    "                       len(x) if len(x)>0 else 0 for x in data['request_text_edit_aware']]\n",
    "\n",
    "def num_unique_words_feature(data):\n",
    "    return [len(set(w for w in x.split())) for x in data['request_text_edit_aware']]\n",
    "\n",
    "def num_punctuations_feature(data):\n",
    "    puncs = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`'\n",
    "    return [sum(x.count(w) for w in puncs) for x in data['request_text_edit_aware']]\n",
    "\n",
    "def requests_with_url(data):\n",
    "    url_regex = re.compile(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\")\n",
    "    return [1 if len([f[0] for f in re.findall(url_regex,str(x))])>0 else 0 for x in data['request_text_edit_aware']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subreddit_freq_with_word(data, col, word):\n",
    "    result = []\n",
    "    for grp_list in data[col]:\n",
    "        counter = 0\n",
    "        for grp in grp_list:\n",
    "            if word.lower() in grp.lower():\n",
    "                counter += 1\n",
    "        result.append(counter)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "# adding some of our own stop words to NLTK's stopword list\n",
    "stopwords_nltk_en = set(stopwords.words('english') + ['though','pizza', 'request', 'hey', 'hi'])\n",
    "# create stop word list for punctuation\n",
    "stopwords_punct = set(punctuation)\n",
    "# combine the three stopwords lists\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "def preprocess_text_tfid(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: str , i.e. list of lemmas\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # remove email addresses\n",
    "    text = re.sub('\\S+@\\S+', '', text)\n",
    "    # convert contractions to their full form\n",
    "    #text = contractions.fix(text)\n",
    "    # remove special characters after lemmatizing words\n",
    "    processed_list = [re.sub('[^A-Za-z0-9]+', '', word) for word in lemmatize_sent(text)\n",
    "              # remove words is they're on the stop words list or is a digit\n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]\n",
    "    # turn list of words into a string to that the output is the same format as the input\n",
    "    return ' '.join(processed_list) \n",
    "\n",
    "def preprocess_text_pmi(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # remove email addresses\n",
    "    text = re.sub('\\S+@\\S+', '', text)\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_nmf_features(data, data_val, col, n_topic, ngram):\n",
    "    train_vectorizer = TfidfVectorizer(analyzer = 'word', \n",
    "                                   preprocessor = preprocess_text_tfid, \n",
    "                                   ngram_range = (ngram,ngram), min_df = 2, max_df = 0.95)\n",
    "\n",
    "    train_vect = train_vectorizer.fit_transform(data[col])\n",
    "    val_vect = train_vectorizer.transform(data_val[col])\n",
    "\n",
    "    train_topic_model = NMF(n_components = n_topic, \n",
    "                            alpha = .1, \n",
    "                            l1_ratio = 0.5).fit(train_vect)\n",
    "\n",
    "    train_topic_feat = train_topic_model.transform(train_vect)\n",
    "    val_topic_feat = train_topic_model.transform(val_vect)\n",
    "    \n",
    "    new_colnames = ['nmf_' + str(ngram) + 'gram_f_'+str(i+1) for i in range(train_topic_feat.shape[1])]\n",
    "    df_train = pd.DataFrame(train_topic_feat, columns=new_colnames)\n",
    "    df_val = pd.DataFrame(val_topic_feat, columns=new_colnames)\n",
    "    \n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_per_category(v, out):\n",
    "    result_true, result_false, result_vocab = {}, {}, {}\n",
    "    for ind, line in enumerate(v):\n",
    "        #print(ind)\n",
    "        #print(out[ind])\n",
    "        trf_line = preprocess_text_pmi(line) # preprocess and lammatized title text\n",
    "        for elem in trf_line:\n",
    "            result_vocab[elem] = result_vocab.get(elem, 0) + 1\n",
    "            \n",
    "            if out[ind]:\n",
    "                result_true[elem] = result_true.get(elem, 0) + 1\n",
    "            else: \n",
    "                result_false[elem] = result_false.get(elem, 0) + 1\n",
    "    \n",
    "    return result_true, result_false, result_vocab\n",
    "\n",
    "def pmi(inclass_freq, total_freq, class_freq, min_count=10):\n",
    "    \"\"\"\n",
    "    inclass_freq (dict): dictionary containing inclass frequency for a given word\n",
    "    total_freq (dict): dictionary containing total frequency for a given word\n",
    "    class_freq (float): class prevalence\n",
    "    min_count (int): minimum frequency for pmi calculation\n",
    "    \"\"\"\n",
    "    pmi = {}\n",
    "    for word, freq in total_freq.items():\n",
    "        if freq < min_count:\n",
    "            continue\n",
    "        pmi[word] = np.log((inclass_freq.get(word, 0) + 0.001) * 1.0 / (class_freq * freq))\n",
    "    return pmi\n",
    "\n",
    "def get_counts(data, text_colname, outcome_colname):\n",
    "    return count_words_per_category(data[text_colname], \n",
    "                                    list(data[outcome_colname]))\n",
    "\n",
    "def get_top_k_from_dict(d, k):\n",
    "    return sorted(d.items(), key=lambda kv: kv[1])[-k:]\n",
    "\n",
    "def get_words(sorted_dict):\n",
    "    return [w[0] for w in sorted_dict]\n",
    "\n",
    "def calculate_pmi(data, text_colname, outcome_colname, class_freq_true, top_k):\n",
    "    count_class_true, count_class_false, count_total = get_counts(data = data, \n",
    "                                                         text_colname=text_colname, \n",
    "                                                         outcome_colname=outcome_colname) \n",
    "    pmi_true = pmi(inclass_freq=count_class_true, \n",
    "                   total_freq=count_total, \n",
    "                   class_freq=class_freq_true)\n",
    "    \n",
    "    pmi_false = pmi(inclass_freq=count_class_false, \n",
    "                   total_freq=count_total, \n",
    "                   class_freq=1-class_freq_true)\n",
    "    \n",
    "    pmi_true_top_k = get_top_k_from_dict(pmi_true, k=top_k)\n",
    "    pmi_false_top_k = get_top_k_from_dict(pmi_false, k=top_k)\n",
    "    \n",
    "    pmi_true_words = get_words(pmi_true_top_k)\n",
    "    pmi_false_words = get_words(pmi_false_top_k)\n",
    "    \n",
    "    return pmi_true_words, pmi_false_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_feature(word, vector, wv):\n",
    "    result = []\n",
    "    candidates = set([w[0] for w in wv.most_similar(word, topn=20)] + [word])\n",
    "    for line in vector:\n",
    "        if line != line:\n",
    "            result.append(0)\n",
    "            continue\n",
    "        trf_line = preprocess_text_pmi(line)\n",
    "        if len(candidates.intersection(set(trf_line))) > 0:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\n",
    "\n",
    "def text_feature_addition(word_list, df, colname, id_str, wv):\n",
    "    for ind, word in enumerate(word_list):\n",
    "        print(ind, ' : ', word)\n",
    "        f_name = id_str + word\n",
    "        feat = get_binary_feature(word, df[colname], wv)\n",
    "        df[f_name] = feat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_categories(data, col):\n",
    "    # understand if the given categories, gratitude or reciprocity is expressed in the raw text\n",
    "    \n",
    "    # Categories\n",
    "    desire = re.compile(r'(friend|party|birthday|boyfriend|girlfriend|date|drinks|drunk|wasted|invite|invited|celebrate|celebrating|game|games|movie|beer|crave|craving)', re.IGNORECASE)\n",
    "    family = re.compile(r'(husband|wife|family|parent|parents|mother|father|mom|mum|son|dad|daughter)', re.IGNORECASE)\n",
    "    job = re.compile(r'(job|unemployment|employment|hire|hired|fired|interview|work|paycheck)', re.IGNORECASE)\n",
    "    money = re.compile(r'(money|bill|bills|rent|bank|account|paycheck|due|broke|bills|deposit|cash|dollar|dollars|bucks|paid|payed|buy|check|spent|financial|poor|loan|credit|budget|day|now| \\\n",
    "        time|week|until|last|month|tonight|today|next|night|when|tomorrow|first|after|while|before|long|hour|Friday|ago|still|due|past|soon|current|years|never|till|yesterday|morning|evening)', re.IGNORECASE)\n",
    "    student = re.compile(r'(college|student|university|finals|study|studying|class|semester|school|roommate|project|tuition|dorm)', re.IGNORECASE)\n",
    "    # Gratitude\n",
    "    gratitude = re.compile(r'(thank|thanks|thankful|appreciate|grateful|gratitude|advance)',re.IGNORECASE)\n",
    "    # Reciprocity\n",
    "    reciprocity = re.compile(r'(pay it forward|pay forward|paid it forward|pay the act forward|pay the favor back|paying it forward|pay this forward|pay pizza forward|pay back|pay it back|pay you back|return the favor|return the favour|pay a pizza forward|repay)',re.IGNORECASE)\n",
    "    \n",
    "    data['desire_category'] = data[col].apply(lambda x: len(desire.findall(x)))\n",
    "    data['family_category'] = data[col].apply(lambda x: len(family.findall(x)))\n",
    "    data['job_category'] = data[col].apply(lambda x: len(job.findall(x)))\n",
    "    data['money_category'] = data[col].apply(lambda x: len(money.findall(x)))\n",
    "    data['student_category'] = data[col].apply(lambda x: len(student.findall(x)))\n",
    "    data['gratitude'] = data[col].apply(lambda x: np.where(len(gratitude.findall(x)) > 0, 1, 0))    \n",
    "    data['reciprocity'] = data[col].apply(lambda x: np.where(len(reciprocity.findall(x))> 0, 1, 0))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def basic_sentiment(data, col):\n",
    "    def is_positive(text):\n",
    "        if not(text.strip()):\n",
    "            return 0\n",
    "        return mean([sia.polarity_scores(sentence)[\"compound\"] for sentence in nltk.sent_tokenize(text)]) > 0 \n",
    "    \n",
    "    def total_score(text):\n",
    "        if not(text.strip()):\n",
    "            return 0\n",
    "        return sum([sia.polarity_scores(sentence)[\"compound\"] for sentence in nltk.sent_tokenize(text)])\n",
    "    \n",
    "    def avg_score(text):\n",
    "        if not(text.strip()):\n",
    "            return 0\n",
    "        return mean([sia.polarity_scores(sentence)[\"compound\"] for sentence in nltk.sent_tokenize(text)])\n",
    "    \n",
    "    pos_bool, tot_scr, avg_scr = [], [], []\n",
    "    \n",
    "    for row in range(len(data)):\n",
    "        text = data.loc[row, col]\n",
    "        pos_bool.append(is_positive(text))\n",
    "        tot_scr.append(total_score(text))\n",
    "        avg_scr.append(avg_score(text))\n",
    "    \n",
    "    data.loc[:,'positive_sentiment_bool'] = pos_bool\n",
    "    data.loc[:,'total_sentiment_score'] = tot_scr\n",
    "    data.loc[:,'average_sentiment_score'] = avg_scr\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec\n",
    "def get_signal(model, i):\n",
    "    try:\n",
    "        return model[i].any()\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "def output_df(trained_model, is_training=True, data=None):\n",
    "    result = []\n",
    "    if is_training:\n",
    "        counter = 0\n",
    "        while get_signal(trained_model, counter):\n",
    "            result.append(list(trained_model[counter]))\n",
    "            counter+=1\n",
    "    else:\n",
    "        result = data\n",
    "    \n",
    "    col_names = ['d2v_'+str(i+1) for i in range(trained_model.vector_size)]\n",
    "    return pd.DataFrame(result, columns=col_names)\n",
    "\n",
    "def create_corpus(data, col, only_tokens=False):\n",
    "    for row in range(len(data)):\n",
    "        line = data.loc[row, col]\n",
    "        tokens = gensim.utils.simple_preprocess(line, deacc=True)\n",
    "        if only_tokens:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [row])\n",
    "            \n",
    "def train_doc2_vec(data, col):\n",
    "    corpus = list(create_corpus(data, col))\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=25, min_count=2, epochs=50)\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "def get_training_vectors(trained_model):\n",
    "    return output_df(trained_model)\n",
    "\n",
    "def infer_vectors(data, col, trained_model):\n",
    "    corpus = list(create_corpus(data, col, only_tokens=True))\n",
    "    vocab = list(trained_model.wv.vocab.keys())\n",
    "    result = []\n",
    "    for row in corpus:\n",
    "        input_text = [t for t in row if t in vocab]\n",
    "        result.append(trained_model.infer_vector(input_text))\n",
    "    return output_df(trained_model, is_training=False, data=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = num_subreddit_cuts(data = train, \n",
    "                        col = 'requester_number_of_subreddits_at_request', \n",
    "                        grid = [-np.inf, 0, 5, 10, 50, 100, np.inf], \n",
    "                        lab = ['0. 0','1. 1-5', '2. 5-10', '3. 10-50', '4. 50-100', '5. 100+'])\n",
    "\n",
    "v1 = num_subreddit_cuts(data = val, \n",
    "                        col = 'requester_number_of_subreddits_at_request', \n",
    "                        grid = [-np.inf, 0, 5, 10, 50, 100, np.inf], \n",
    "                        lab = ['0. 0','1. 1-5', '2. 5-10', '3. 10-50', '4. 50-100', '5. 100+'])\n",
    "\n",
    "train.loc[:,'num_subreddits_req'] = t1\n",
    "val.loc[:,'num_subreddits_req'] = v1\n",
    "\n",
    "train = num_subreddit_dummy(data = train, colname= 'num_subreddits_req')\n",
    "val = num_subreddit_dummy(data = val, colname= 'num_subreddits_req')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = num_posts_roap(data=train, colname='requester_number_of_posts_on_raop_at_request', id_colname='binary')\n",
    "val = num_posts_roap(data=val, colname='requester_number_of_posts_on_raop_at_request', id_colname='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_request_hour_feature(data = train, time_col='unix_timestamp_of_request', interval=4)\n",
    "val = add_request_hour_feature(data = val, time_col='unix_timestamp_of_request', interval=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,'title_length'] = simple_token_count(train.request_title)\n",
    "train.loc[:,'text_length'] = simple_token_count(train.request_text_edit_aware)\n",
    "val.loc[:,'title_length'] = simple_token_count(val.request_title)\n",
    "val.loc[:,'text_length'] = simple_token_count(val.request_text_edit_aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,'active_random_subr'] = find_subreddit_freq_with_word(data=train, col='requester_subreddits_at_request', word='random')\n",
    "train.loc[:,'active_raop_subr'] = find_subreddit_freq_with_word(data=train, col='requester_subreddits_at_request', word='Random_Acts_Of_Pizza')\n",
    "train.loc[:,'active_food_subr'] = find_subreddit_freq_with_word(data=train, col='requester_subreddits_at_request', word='food')\n",
    "\n",
    "val.loc[:,'active_random_subr'] = find_subreddit_freq_with_word(data=val, col='requester_subreddits_at_request', word='random')\n",
    "val.loc[:,'active_raop_subr'] = find_subreddit_freq_with_word(data=val, col='requester_subreddits_at_request', word='Random_Acts_Of_Pizza')\n",
    "val.loc[:,'active_food_subr'] = find_subreddit_freq_with_word(data=val, col='requester_subreddits_at_request', word='food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,'lengths_text'] = lengths_feature(data=train)\n",
    "train.loc[:,'num_caps_text'] = num_caps_feature(data=train)\n",
    "train.loc[:,'capsvslen_text'] = caps_vs_lengths_feature(data=train)\n",
    "train.loc[:,'num_unique_words_text'] = num_unique_words_feature(data=train)\n",
    "train.loc[:,'num_puncs_text'] = num_punctuations_feature(data=train)\n",
    "train.loc[:,'num_req_w_url_text'] = requests_with_url(data=train)\n",
    "\n",
    "val.loc[:,'lengths_text'] = lengths_feature(data=val)\n",
    "val.loc[:,'num_caps_text'] = num_caps_feature(data=val)\n",
    "val.loc[:,'capsvslen_text'] = caps_vs_lengths_feature(data=val)\n",
    "val.loc[:,'num_unique_words_text'] = num_unique_words_feature(data=val)\n",
    "val.loc[:,'num_puncs_text'] = num_punctuations_feature(data=val)\n",
    "val.loc[:,'num_req_w_url_text'] = requests_with_url(data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_nmf_train, bigram_nmf_val = n_gram_nmf_features(data=train, data_val=val, \n",
    "                                                       col='request_text_edit_aware', n_topic=5, ngram=2)\n",
    "\n",
    "unigram_nmf_train, unigram_nmf_val = n_gram_nmf_features(data=train, data_val=val, \n",
    "                                                       col='request_text_edit_aware', n_topic=10, ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True, drop=True)\n",
    "val.reset_index(inplace=True, drop=True)\n",
    "train = pd.concat([train, bigram_nmf_train, unigram_nmf_train], axis=1)\n",
    "val = pd.concat([val, bigram_nmf_val, unigram_nmf_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = manual_categories(data=train, col='request_text_edit_aware')\n",
    "val = manual_categories(data=val, col='request_text_edit_aware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = basic_sentiment(data=train, col='request_text_edit_aware')\n",
    "val = basic_sentiment(data=val, col='request_text_edit_aware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec \n",
    "m = train_doc2_vec(train, 'request_text_edit_aware')\n",
    "d2v_train = get_training_vectors(m)\n",
    "d2v_val = infer_vectors(val, 'request_text_edit_aware', m)\n",
    "\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "val.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train = pd.concat([train, d2v_train], axis=1)\n",
    "val = pd.concat([val, d2v_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vect = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_class_pmi_words_title, false_class_pmi_words_title = calculate_pmi(data = train, \n",
    "                                                                        text_colname='request_title', \n",
    "                                                                        outcome_colname='requester_received_pizza', \n",
    "                                                                        class_freq_true=0.24505, \n",
    "                                                                        top_k=10)\n",
    "\n",
    "true_class_pmi_words_text, false_class_pmi_words_text = calculate_pmi(data = train, \n",
    "                                                                      text_colname='request_text_edit_aware', \n",
    "                                                                      outcome_colname='requester_received_pizza', \n",
    "                                                                      class_freq_true=0.24505, \n",
    "                                                                      top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = text_feature_addition(word_list=false_class_pmi_words_text, \n",
    "                                    df=train, \n",
    "                                    colname='request_text_edit_aware', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='text_false_')\n",
    "\n",
    "train = text_feature_addition(word_list=true_class_pmi_words_text, \n",
    "                                    df=train, \n",
    "                                    colname='request_text_edit_aware', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='text_true_')\n",
    "\n",
    "train = text_feature_addition(word_list=false_class_pmi_words_title, \n",
    "                                    df=train, \n",
    "                                    colname='request_title', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='title_false_')\n",
    "\n",
    "train = text_feature_addition(word_list=true_class_pmi_words_title, \n",
    "                                    df=train, \n",
    "                                    colname='request_title', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='title_true_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = text_feature_addition(word_list=false_class_pmi_words_text, \n",
    "                                    df=val, \n",
    "                                    colname='request_text_edit_aware', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='text_false_')\n",
    "\n",
    "val = text_feature_addition(word_list=true_class_pmi_words_text, \n",
    "                                    df=val, \n",
    "                                    colname='request_text_edit_aware', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='text_true_')\n",
    "\n",
    "val = text_feature_addition(word_list=false_class_pmi_words_title, \n",
    "                                    df=val, \n",
    "                                    colname='request_title', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='title_false_')\n",
    "\n",
    "val = text_feature_addition(word_list=true_class_pmi_words_title, \n",
    "                                    df=val, \n",
    "                                    colname='request_title', \n",
    "                                    wv=w_vect, \n",
    "                                    id_str='title_true_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['requester_received_pizza', 'request_title', \n",
    "                'request_text_edit_aware', 'requester_subreddits_at_request']\n",
    "Xcols=[c for c in train.columns if c not in exclude_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grids():\n",
    "    param_grids = {}\n",
    "    param_grids['svm'] = {\n",
    "        #'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'C': [0.1, 1, 100] ,\n",
    "        'gamma': [0.0001, 0.001, 0.1, 1, 5]}\n",
    "\n",
    "    param_grids['knn'] = {\n",
    "        'n_neighbors':[5,6,7,8,9,10],\n",
    "        'leaf_size':[1,2,3,5],\n",
    "        'weights':['uniform', 'distance'],\n",
    "        'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n",
    "        'n_jobs':[-1]}\n",
    "\n",
    "    param_grids['rf'] = {\n",
    "        'n_estimators': [int(x) for x in np.linspace(200, 1000, 5)],\n",
    "        'max_depth': [int(x) for x in np.linspace(1, 10, 1)],\n",
    "        'max_features': ['auto', 'log2', 8, 10, 15, 20],\n",
    "        'min_samples_split': [int(x) for x in np.linspace(4, 10, 7)],\n",
    "        'bootstrap': [True, False], \n",
    "        'class_weight': [None, 'balanced', 'balanced_subsample']}\n",
    "\n",
    "    param_grids['et'] = {\n",
    "        'n_estimators': range(50,126,25),\n",
    "        'max_features': range(50,401,50),\n",
    "        'min_samples_leaf': range(20,50,5),\n",
    "        'min_samples_split': range(15,36,5)}\n",
    "\n",
    "    param_grids['xgb'] = {\n",
    "        'learning_rate'    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "        'max_depth'        : [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "        'min_child_weight' : [1, 3, 5, 7],\n",
    "        'gamma'            : [0.0, 0.1, 0.2 , 0.3, 0.4],\n",
    "        'colsample_bytree' : [0.3, 0.4, 0.5 , 0.7]}\n",
    "\n",
    "    param_grids['ada'] = {\n",
    "        'n_estimators': [10, 50, 100, 500],\n",
    "        'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "    param_grids['gbm'] = {\n",
    "        'n_estimators': [10, 100, 1000],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'subsample': [0.5, 0.7, 1.0],\n",
    "        'max_depth': [3, 7, 9]\n",
    "    }\n",
    "\n",
    "    param_grids['lr'] = {\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "        'penalty': ['l2'],\n",
    "        'C': [100, 10, 1.0, 0.1, 0.01]}\n",
    "\n",
    "    param_grids['nb'] = {\n",
    "        'var_smoothing': np.logspace(0,-9, num=10)}\n",
    "\n",
    "    param_grids['nn'] = {\n",
    "        'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']}\n",
    "\n",
    "    param_grids['bag'] = {'n_estimators': [10, 100, 1000]}\n",
    "    \n",
    "    return param_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, model_class, X, y, param_grids):\n",
    "    \n",
    "    cv = GridSearchCV(model, param_grid=param_grids, cv=5, scoring='roc_auc', n_jobs=-1, verbose=3)\n",
    "\n",
    "    cv.fit(X,y)\n",
    "    \n",
    "    print(cv.best_params_)\n",
    "    return cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_classes():\n",
    "    model_classes = {}\n",
    "    model_classes['knn'] = KNeighborsClassifier()\n",
    "    model_classes['svm'] = sklearn.svm.SVC()\n",
    "    model_classes['rf'] = RandomForestClassifier()\n",
    "    model_classes['et'] = ExtraTreesClassifier()\n",
    "    model_classes['xgb'] = xgb.XGBClassifier(random_state=1)\n",
    "    model_classes['ada'] = AdaBoostClassifier(random_state=1)\n",
    "    model_classes['gbm'] = GradientBoostingClassifier(random_state=1)\n",
    "    model_classes['lr'] = LogisticRegression()\n",
    "    model_classes['nb'] = GaussianNB()\n",
    "    model_classes['nn'] = MLPClassifier()\n",
    "    model_classes['bag'] = BaggingClassifier()\n",
    "    return model_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X,y, model_class='svm'):\n",
    "    if(model_class=='xgb'):\n",
    "        for col in X:\n",
    "            if(X[col].dtype=='uint8'):\n",
    "                X[col] = np.int8(X[col])\n",
    "            if(X[col].dtype=='object'):\n",
    "                X[col] = pd.to_numeric(X[col])\n",
    "    return model_eval(get_model_classes()[model_class], model_class, X, y, get_param_grids()[model_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_include(X,y,model_types_to_include):\n",
    "    models = []\n",
    "    for mt in model_types_to_include:\n",
    "        models.append(build_model(X,y,mt))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackX(models, X):\n",
    "    stackX = None\n",
    "    for model in models:\n",
    "        # make prediction\n",
    "        yhat = model.predict(X)\n",
    "        # stack predictions into [predictions, rows, members]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.vstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x predictions]\n",
    "    stackX = np.transpose(stackX)#stackX.reshape((stackX.shape[1], stackX.shape[1]*stackX.shape[0]))\n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ensemble(trainX,trainY,evaluateX,evaluateY,\n",
    "                   model_types_to_include=['svm','knn','rf','et','xgb','ada','gbm','lr','nb','nn','bag']):\n",
    "    \n",
    "    models = models_to_include(trainX,trainY,model_types_to_include)\n",
    "    \n",
    "    stack = stackX(models, evaluateX)\n",
    "    #fit model on the stacked train X\n",
    "    model_on_stackedX = build_model(stack, val[y], model_class='nn')\n",
    "    model_on_stackedX.fit(stack, evaluateY)\n",
    "    stacked_ensemble_pred = model_on_stackedX.predict(stack)\n",
    "    print(classification_report(y_true=evaluateY, y_pred=stacked_ensemble_pred))\n",
    "    return model_on_stackedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_ensemble_model = build_ensemble(train[Xcols], train[y], val[Xcols], val[y],\n",
    "                   model_types_to_include=['svm','knn', 'rf','et','xgb','ada','gbm','lr','nb','nn', 'bag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probs = rf_final.predict_proba(val[Xcols])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rf_final.feature_importances_, index=Xcols)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate roc curves\n",
    "precision, recall, thresholds = precision_recall_curve(val[y], prediction_probs)\n",
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.nanargmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = 0.24505\n",
    "plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='RF')\n",
    "plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = prediction_probs>=0.253172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=val[y], y_pred=pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = val[['requester_received_pizza', 'request_title', 'request_text_edit_aware']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data.loc[:,'prediction'] = prediction_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data['predicted_lable'] = prediction_probs>=0.1551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(base_data.requester_received_pizza, base_data.predicted_lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data[(base_data['requester_received_pizza']) & ~(base_data['predicted_lable'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_data[(~base_data['requester_received_pizza']) & (base_data['predicted_lable'])].sort_values(by='prediction', \n",
    "                                                                                                 ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_data[(base_data['requester_received_pizza']) & (base_data['predicted_lable'])].sort_values(by='prediction', \n",
    "                                                                                                 ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
