{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "# conda install textblob -c conda-forge\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aliceye/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aliceye/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/aliceye/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aliceye/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_data = json.load(open('train.json'))\n",
    "train_df = pd.json_normalize(train_json_data)\n",
    "\n",
    "test_json_data = json.load(open('test.json'))\n",
    "test_df = pd.json_normalize(test_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
       "       'request_title', 'requester_account_age_in_days_at_request',\n",
       "       'requester_days_since_first_post_on_raop_at_request',\n",
       "       'requester_number_of_comments_at_request',\n",
       "       'requester_number_of_comments_in_raop_at_request',\n",
       "       'requester_number_of_posts_at_request',\n",
       "       'requester_number_of_posts_on_raop_at_request',\n",
       "       'requester_number_of_subreddits_at_request',\n",
       "       'requester_subreddits_at_request',\n",
       "       'requester_upvotes_minus_downvotes_at_request',\n",
       "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
       "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_cols = ['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
    "       'request_title', 'requester_account_age_in_days_at_request',\n",
    "       'requester_days_since_first_post_on_raop_at_request',\n",
    "       'requester_number_of_comments_at_request',\n",
    "       'requester_number_of_comments_in_raop_at_request',\n",
    "       'requester_number_of_posts_at_request',\n",
    "       'requester_number_of_posts_on_raop_at_request',\n",
    "       'requester_number_of_subreddits_at_request',\n",
    "       'requester_subreddits_at_request',\n",
    "       'requester_upvotes_minus_downvotes_at_request',\n",
    "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
    "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc', 'requester_received_pizza']\n",
    "short_train_df = train_df[available_cols]\n",
    "short_train_df.columns\n",
    "no_pizza_df = short_train_df[short_train_df.requester_received_pizza == False]\n",
    "yes_pizza_df = short_train_df[short_train_df.requester_received_pizza == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english') + ['though','pizza', 'request', 'hey', 'hi'])\n",
    "stopwords_punct = set(punctuation)\n",
    "# combine the stopwords.\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions \n",
    "# import sys  \n",
    "# !{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    processed_list = [word for word in lemmatize_sent(contractions.fix(re.sub(r'http\\S+', '', text)))\n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]\n",
    "    processed_text = ' '.join(processed_list) \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're in Tampa Florida...moving to Ybor on Friday. My email is usfstuff@yahoo.com, since I'm having major trouble figuring out reddit's message system.\\n\\n\\nMy two roommates and I have found a new place to live and are moving in on Friday. The rent is $1100 including utitilies and internet. My 2 roomies have very nicely offered for me to just pay $200 a month until I have a job. I am so thankful to have them as friends! This month all of us are extremely poor from paying $2000 to landlord (first+last rent+deposit). We also had to pay $310 for TECO to turn on the electricity.\\n\\n\\nI would just really love to do something nice for them and pick them up a pizza and a sub/pzone. My one roomie doesn't love pizza and she only ever gets a sub/pzone haha. Using a few coupons, I can get 8 pizza rollers, 1 large pizza, and a pzone for $11 from Pizza Hut. If anyone can help me out I would be so grateful! Pizza Hut is just the cheapest, if you have something else in mind let me know. I have an interview on Thursday so keep your fingers crossed!\\n\\n\\nThe pic for our huge TECO deposit :/\\nhttp://imgur.com/B0uoo\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_train_df['request_text_edit_aware'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tampa florida ... move ybor friday email usfstuff yahoo.com major trouble figure reddit 's message system roommate find place live move friday rent include utitilies internet roomy nicely offer pay month job thankful friend month extremely poor pay landlord first+last rent+deposit pay teco turn electricity love nice pick sub/pzone roomie love sub/pzone haha coupon roller large pzone hut grateful hut cheap mind interview thursday finger cross pic huge teco deposit\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(short_train_df['request_text_edit_aware'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10166666666666666\n",
      "0.31333333333333335\n"
     ]
    }
   ],
   "source": [
    "# polarity ranges from -1 to 1, with -1 being negative and 1 being positive.\n",
    "print(TextBlob(short_train_df['request_text_edit_aware'][0]).polarity)\n",
    "# subjectivity which ranges from 0 to 1, with 0 being objective and 1 being subjective.\n",
    "print(TextBlob(short_train_df['request_text_edit_aware'][0]).subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bigram vectorizer and \n",
    "# override the analyzer totally with the preprocess_text()\n",
    "count_vect = CountVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (2, 2))\n",
    "short_train_df_cv = count_vect.fit_transform(short_train_df['request_text_edit_aware'])\n",
    "\n",
    "# count frequency of ngrams\n",
    "count_values = short_train_df_cv.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = count_vect.vocabulary_\n",
    "df_bigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'count', 1:'bigram'})\n",
    "df_bigram['polarity'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_bigram['subjective'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>bigram</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471</td>\n",
       "      <td>pay forward</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>college student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142</td>\n",
       "      <td>bank account</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129</td>\n",
       "      <td>lose job</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>return favor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>121</td>\n",
       "      <td>sob story</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>94</td>\n",
       "      <td>papa john</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92</td>\n",
       "      <td>pay back</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89</td>\n",
       "      <td>pay rent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>86</td>\n",
       "      <td>random act</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frequency           bigram  polarity  subjective\n",
       "0        471      pay forward       0.0         0.0\n",
       "1        147  college student       0.0         0.0\n",
       "2        142     bank account       0.0         0.0\n",
       "3        129         lose job       0.0         0.0\n",
       "4        128     return favor       0.0         0.0\n",
       "5        121        sob story       0.0         0.0\n",
       "6         94        papa john       0.0         0.0\n",
       "7         92         pay back       0.0         0.0\n",
       "8         89         pay rent       0.0         0.0\n",
       "9         86       random act      -0.5         0.5"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bigram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bigram vectorizer on those who did not get pizza\n",
    "count_vect = CountVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (2, 2))\n",
    "short_train_df_cv = count_vect.fit_transform(no_pizza_df['request_text_edit_aware'])\n",
    "\n",
    "# count frequency of ngrams\n",
    "count_values = short_train_df_cv.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = count_vect.vocabulary_\n",
    "df_bigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'count', 1:'bigram'})\n",
    "df_bigram['polarity'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_bigram['subjective'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>bigram</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317</td>\n",
       "      <td>pay forward</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>college student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98</td>\n",
       "      <td>bank account</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>return favor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>sob story</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88</td>\n",
       "      <td>lose job</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>random act</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63</td>\n",
       "      <td>pay rent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62</td>\n",
       "      <td>papa john</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>hey guy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frequency           bigram  polarity  subjective\n",
       "0        317      pay forward       0.0         0.0\n",
       "1        102  college student       0.0         0.0\n",
       "2         98     bank account       0.0         0.0\n",
       "3         94     return favor       0.0         0.0\n",
       "4         90        sob story       0.0         0.0\n",
       "5         88         lose job       0.0         0.0\n",
       "6         64       random act      -0.5         0.5\n",
       "7         63         pay rent       0.0         0.0\n",
       "8         62        papa john       0.0         0.0\n",
       "9         60          hey guy       0.0         0.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bigram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bigram vectorizer on those who did get pizza\n",
    "count_vect = CountVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (2, 2))\n",
    "short_train_df_cv = count_vect.fit_transform(yes_pizza_df['request_text_edit_aware'])\n",
    "\n",
    "# count frequency of ngrams\n",
    "count_values = short_train_df_cv.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = count_vect.vocabulary_\n",
    "df_bigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'count', 1:'bigram'})\n",
    "df_bigram['polarity'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_bigram['subjective'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>bigram</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154</td>\n",
       "      <td>pay forward</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>college student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>bank account</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>lose job</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>return favor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>pay back</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>papa john</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>sob story</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>pay friday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>find job</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frequency           bigram  polarity  subjective\n",
       "0        154      pay forward       0.0         0.0\n",
       "1         45  college student       0.0         0.0\n",
       "2         44     bank account       0.0         0.0\n",
       "3         41         lose job       0.0         0.0\n",
       "4         34     return favor       0.0         0.0\n",
       "5         34         pay back       0.0         0.0\n",
       "6         32        papa john       0.0         0.0\n",
       "7         31        sob story       0.0         0.0\n",
       "8         27       pay friday       0.0         0.0\n",
       "9         27         find job       0.0         0.0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bigram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trigram vectorizer\n",
    "cv_trigram = CountVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (3, 3))\n",
    "cv_trigram_fit = cv_trigram.fit_transform(short_train_df['request_text_edit_aware'])\n",
    "\n",
    "# count frequency of ngrams\n",
    "count_values = cv_trigram_fit.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "trigram_vocab = cv_trigram.vocabulary_\n",
    "df_trigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in trigram_vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'count', 1:'trigram'})\n",
    "df_trigram['polarity'] = df_trigram['trigram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_trigram['subjective'] = df_trigram['trigram'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>trigram</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>promise pay forward</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>long story short</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>pay forward pay</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>broke college student</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>part time job</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>work part time</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>love pay forward</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18</td>\n",
       "      <td>long time lurker</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>pay forward future</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>live paycheck paycheck</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frequency                 trigram  polarity  subjective\n",
       "0         54     promise pay forward  0.000000       0.000\n",
       "1         45        long story short -0.025000       0.350\n",
       "2         31         pay forward pay  0.000000       0.000\n",
       "3         24   broke college student  0.000000       0.000\n",
       "4         19           part time job  0.000000       0.000\n",
       "5         18          work part time  0.000000       0.000\n",
       "6         18        love pay forward  0.500000       0.600\n",
       "7         18        long time lurker -0.050000       0.400\n",
       "8         17      pay forward future  0.000000       0.125\n",
       "9         17  live paycheck paycheck  0.136364       0.500"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trigram[df_trigram.frequency>10][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: pay forward, bank account, return favor\n",
      "Topic #1: pay forward, college student, lose job\n",
      "Topic #2: pay forward, bank account, pay week\n",
      "Topic #3: pay forward, sob story, lose job\n",
      "Topic #4: pay forward, college student, bank account\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topics for CV bigrams\n",
    "cv_bigram = CountVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (2, 2))\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "pipe = make_pipeline(cv_bigram, lda)\n",
    "pipe.fit(short_train_df['request_text_edit_aware'])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(lda, cv_bigram.get_feature_names(), n_top_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: promise pay forward, broke college student, make ends meet\n",
      "Topic #1: long story short, promise pay forward, broke college student\n",
      "Topic #2: long story short, promise pay forward, long time lurker\n",
      "Topic #3: poor college student, long story short, promise pay forward\n",
      "Topic #4: pay forward pay, promise pay forward, long story short\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topics for CV trigrams\n",
    "cv_trigram = CountVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (3, 3))\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "pipe = make_pipeline(cv_trigram, lda)\n",
    "pipe.fit(short_train_df['request_text_edit_aware'])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(lda, cv_trigram.get_feature_names(), n_top_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bigram TDIFD vectorizer \n",
    "tf_bigram = TfidfVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (2, 2))\n",
    "short_train_df_tf_bigram = tf_bigram.fit_transform(short_train_df['request_text_edit_aware'])\n",
    "\n",
    "# average frequency of ngrams\n",
    "avg_values = short_train_df_tf_bigram.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = tf_bigram.vocabulary_\n",
    "df_bigram = pd.DataFrame(sorted([(avg_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'sum frequency', 1:'bigram'})\n",
    "df_bigram['polarity'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_bigram['subjective'] = df_bigram['bigram'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum frequency</th>\n",
       "      <th>bigram</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.409068</td>\n",
       "      <td>pay forward</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.213445</td>\n",
       "      <td>college student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.103988</td>\n",
       "      <td>return favor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.499367</td>\n",
       "      <td>sob story</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.310112</td>\n",
       "      <td>bank account</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.238537</td>\n",
       "      <td>lose job</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.831305</td>\n",
       "      <td>papa john</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.485988</td>\n",
       "      <td>pay back</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.047579</td>\n",
       "      <td>couple day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.012277</td>\n",
       "      <td>pay week</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sum frequency           bigram  polarity  subjective\n",
       "0      43.409068      pay forward       0.0         0.0\n",
       "1      20.213445  college student       0.0         0.0\n",
       "2      17.103988     return favor       0.0         0.0\n",
       "3      16.499367        sob story       0.0         0.0\n",
       "4      15.310112     bank account       0.0         0.0\n",
       "5      14.238537         lose job       0.0         0.0\n",
       "6      11.831305        papa john       0.0         0.0\n",
       "7      11.485988         pay back       0.0         0.0\n",
       "8      10.047579       couple day       0.0         0.0\n",
       "9      10.012277         pay week       0.0         0.0"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bigram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bigram TDIFD vectorizer \n",
    "tf_trigram = TfidfVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (3, 3))\n",
    "short_train_df_tf_trigram = tf_trigram.fit_transform(short_train_df['request_text_edit_aware'])\n",
    "\n",
    "# average frequency of ngrams\n",
    "num_values = short_train_df_tf_trigram.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = tf_trigram.vocabulary_\n",
    "df_trigram = pd.DataFrame(sorted([(num_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'num frequency', 1:'trigram'})\n",
    "df_trigram['polarity'] = df_trigram['trigram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_trigram['subjective'] = df_trigram['trigram'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num frequency</th>\n",
       "      <th>trigram</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.074477</td>\n",
       "      <td>promise pay forward</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.217204</td>\n",
       "      <td>broke college student</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.216487</td>\n",
       "      <td>long story short</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.758339</td>\n",
       "      <td>pay forward pay</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.980866</td>\n",
       "      <td>love pay forward</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.972173</td>\n",
       "      <td>poor college student</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.864699</td>\n",
       "      <td>pay forward future</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.806495</td>\n",
       "      <td>pay forward friday</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.780547</td>\n",
       "      <td>gladly pay forward</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.674388</td>\n",
       "      <td>lose job week</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num frequency                trigram  polarity  subjective\n",
       "0       7.074477    promise pay forward     0.000       0.000\n",
       "1       5.217204  broke college student     0.000       0.000\n",
       "2       5.216487       long story short    -0.025       0.350\n",
       "3       4.758339        pay forward pay     0.000       0.000\n",
       "4       2.980866       love pay forward     0.500       0.600\n",
       "5       2.972173   poor college student    -0.400       0.600\n",
       "6       2.864699     pay forward future     0.000       0.125\n",
       "7       2.806495     pay forward friday     0.000       0.000\n",
       "8       2.780547     gladly pay forward     0.500       1.000\n",
       "9       2.674388          lose job week     0.000       0.000"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trigram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: sob story hungry, pay forward pay, pay forward money\n",
      "Topic #1: eat ramen noodle, food money kind, pepperroni mushroom visitor\n",
      "Topic #2: promise pay forward, pay forward pay, hungry college student\n",
      "Topic #3: broke college student, lose job couple, promise pay forward\n",
      "Topic #4: promise pay forward, broke college student, happy pay forward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topics for TFIDF trigrams using LDA\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (3, 3))\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "pipe = make_pipeline(tfidf_vectorizer, lda)\n",
    "pipe.fit(short_train_df['request_text_edit_aware'])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(lda, tfidf_vectorizer.get_feature_names(), n_top_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: broke college student, college student pay, hey broke college\n",
      "Topic #1: promise pay forward, forward pass test, pass test awww\n",
      "Topic #2: pay forward pay, forward pay week, week buy pay\n",
      "Topic #3: pay forward friday, happy pay forward, friday columbus ohio\n",
      "Topic #4: long story short, break college student, hungry college student\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## topics for TFIDF trigrams using NML\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (3, 3))\n",
    "nmf = NMF(n_components=5)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(short_train_df['request_text_edit_aware'])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NML seems to be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: back work sound, pile bad news, careful food fridge\n",
      "Topic #1: pay forward money, bean ramen pasta, rice bean ramen\n",
      "Topic #2: pay forward feel, food house fund, thought give break\n",
      "Topic #3: grateful pay forward, pay forward day, forever grateful pay\n",
      "Topic #4: promise pay forward, life promise pay, people day life\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## topics for TFIDF trigrams using NML for those who did get piza\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (3, 3))\n",
    "nmf = NMF(n_components=5)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(short_train_df.request_text_edit_aware[short_train_df.requester_received_pizza == True])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: broke college student, college student pay, student pay forward\n",
      "Topic #1: promise pay forward, pass test awww, forward pass test\n",
      "Topic #2: pay forward pay, forward pay week, week buy pay\n",
      "Topic #3: sob story hungry, story hungry bore, cry sob story\n",
      "Topic #4: hungry college student, student happy eat, feel donate hungry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## topics for TFIDF trigrams using NML for those who did not get piza\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', preprocessor = preprocess_text, ngram_range = (3, 3))\n",
    "nmf = NMF(n_components=5)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(short_train_df.request_text_edit_aware[short_train_df.requester_received_pizza == False])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "College/student shows up more in not getting a pizza.\n",
    "Being grateful seems to show up in more topics when a pizza is received.\n",
    "Way to make pay if forward like phrases one topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
